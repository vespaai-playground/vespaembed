# Similarity task with LoRA
# Parameter-efficient fine-tuning for similarity scoring

task: similarity
base_model: sentence-transformers/all-MiniLM-L6-v2

data:
  train: ./examples/data/similarity.csv

loss_variant: cosent  # CoSENT often outperforms cosine

training:
  epochs: 3
  batch_size: 32
  learning_rate: 2e-5
  warmup_ratio: 0.1
  weight_decay: 0.01
  fp16: true
  optimizer: adamw_torch
  scheduler: linear
  eval_steps: 500
  save_steps: 500
  logging_steps: 100

output:
  push_to_hub: false

lora:
  enabled: true
  r: 64
  alpha: 128
  dropout: 0.1
  target_modules: [query, key, value, dense]
